
from typing import Annotated, TypedDict, List, Union, Literal
from langgraph.graph import StateGraph, END
from .llm import FastGPTClient
import json
import datetime
import os

# å®šä¹‰ Agent çŠ¶æ€ï¼Œå¢åŠ â€œåæ€æ¬¡æ•°â€å’Œâ€œè§„åˆ’æŒ‡ä»¤â€
class AgentState(TypedDict):
    case_info: str
    language: str
    history: List[dict]
    analysis_result: dict
    planning_instruction: str # è§„åˆ’è€…ä¸‹è¾¾çš„é‡ç‚¹å®¡è®¡æ–¹å‘
    refinement_count: int      # å¾ªç¯ä¿®æ­£æ¬¡æ•°
    is_logical: bool           # é€»è¾‘è‡ªæ´½æ£€æŸ¥ä½

llm = FastGPTClient(
    api_key=os.getenv("FASTGPT_API_KEY", "your-api-key"),
    base_url=os.getenv("FASTGPT_BASE_URL", "https://api.fastgpt.in/api/v1")
)

def get_now():
    return datetime.datetime.now().strftime("%H:%M:%S")

# --- 1. è§„åˆ’èŠ‚ç‚¹ (The Planner) ---
async def planning_node(state: AgentState):
    """æœ€å‰æ²¿åšæ³•ï¼šåœ¨æ‰§è¡Œå‰å…ˆè¿›è¡Œä»»åŠ¡åˆ†è§£å’Œé‡ç‚¹æ ‡è®°"""
    prompt = f"åˆ†ææ­¤æˆ¿äº§æŠµæŠ¼æ¡ˆå·ï¼ŒæŒ‡å‡ºè¯¥æ¡ˆä»¶æœ€æ ¸å¿ƒçš„é£é™©ç»´åº¦ï¼ˆèµ„äº§ã€ç»è¥ã€è´Ÿå€ºã€æ¬ºè¯ˆä¸­çš„å“ªä¸€ä¸ªï¼‰ï¼Œå¹¶ç»™åç»­ä¸“å®¶ä¸‹è¾¾å®¡è®¡æŒ‡ä»¤ï¼š{state['case_info']}"
    plan = await llm.chat_completion(
        system_prompt="ä½ æ˜¯ä¸€åèµ„æ·±é¦–å¸­é£é™©å®˜ï¼Œæ“…é•¿æ¡ˆä»¶é¢„åˆ¤ã€‚è¯·ç»™å‡ºç®€çŸ­çš„å®¡è®¡è§„åˆ’æŒ‡ä»¤ã€‚",
        user_prompt=prompt
    )
    state['planning_instruction'] = plan
    state['history'].append({"role": "System Planner", "content": f"ğŸ¯ è§„åˆ’æŒ‡ä»¤ï¼š{plan}", "timestamp": get_now()})
    return state

# --- 2. ä¸“å®¶èŠ‚ç‚¹ (Enhanced Expert with Planning) ---
async def expert_node_factory(role_key: str, state: AgentState):
    from .graph import ROLE_BRAINS, METHODOLOGIES
    brain = ROLE_BRAINS[role_key]
    method = METHODOLOGIES[role_key]
    
    system_instruction = f"""
    # ä¸“å®¶èº«ä»½ï¼š{brain['name']}
    # å®¡è®¡æ¡†æ¶ï¼š{method['framework']}
    # è§„åˆ’æŒ‡ä»¤ï¼š{state['planning_instruction']}
    
    ä½ å¿…é¡»åœ¨å®¡è®¡ä¸­ä¼˜å…ˆå“åº”â€˜è§„åˆ’æŒ‡ä»¤â€™ä¸­çš„è¦æ±‚ã€‚å¦‚æœè¿™æ˜¯ç¬¬äºŒæ¬¡å®¡è®¡ï¼ˆRefinementï¼‰ï¼Œä½ å¿…é¡»é’ˆå¯¹ä¹‹å‰çš„è´¨ç–‘è¿›è¡Œåé©³æˆ–ä¿®æ­£ã€‚
    """
    
    user_prompt = f"æ¡ˆå·ï¼š{state['case_info']}\nä¸Šä¸‹æ–‡ï¼š{json.dumps(state['history'][-3:], ensure_ascii=False)}"
    content = await llm.chat_completion(system_prompt=system_instruction, user_prompt=user_prompt)
    
    state['history'].append({"role": brain['name'], "content": content, "timestamp": get_now()})
    return state

# --- 3. åŠ¨æ€åæ€èŠ‚ç‚¹ (The Critic/Arbiter) ---
async def arbiter_node(state: AgentState):
    """è£å†³å¹¶æ£€æŸ¥é€»è¾‘è‡ªæ´½æ€§"""
    prompt = f"åŸºäºä»¥ä¸‹è¾©è®ºï¼Œç»™å‡º JSON æŠ¥å‘Šã€‚ç‰¹åˆ«æ³¨æ„ï¼šæ£€æŸ¥ä¸“å®¶æ„è§æ˜¯å¦æœ‰å†²çªã€‚è¾©è®ºï¼š{json.dumps(state['history'], ensure_ascii=False)}"
    
    json_str = await llm.chat_completion(
        system_prompt="ä½ ä¸ä»…æ˜¯è£å†³è€…ï¼Œè¿˜æ˜¯é€»è¾‘æ£€æŸ¥å®˜ã€‚å¦‚æœä¸“å®¶æ„è§å­˜åœ¨ä¸¥é‡æ•°æ®çŸ›ç›¾ï¼Œè¯·åœ¨å›å¤ä¸­åŒ…å« 'LOGIC_ERROR' å…³é”®è¯ã€‚",
        user_prompt=prompt
    )
    
    # æ¨¡æ‹Ÿé€»è¾‘è‡ªæ´½æ€§æ£€æŸ¥
    state['is_logical'] = "LOGIC_ERROR" not in json_str
    
    try:
        start_idx = json_str.find('{')
        end_idx = json_str.rfind('}') + 1
        state['analysis_result'] = json.loads(json_str[start_idx:end_idx])
    except:
        state['is_logical'] = False
        
    state['history'].append({"role": "Final Arbiter", "content": state['analysis_result'].get('summary', 'é€»è¾‘è¯„ä¼°ä¸­...'), "timestamp": get_now()})
    return state

# --- 4. è·¯ç”±é€»è¾‘ (Conditional Edges) ---
def should_continue(state: AgentState) -> Literal["refine", "end"]:
    """å‰æ²¿æ¶æ„æ ¸å¿ƒï¼šæ ¹æ®é€»è¾‘è‡ªæ´½æ€§å’Œé‡è¯•æ¬¡æ•°å†³å®šæ˜¯å¦è¿›å…¥ä¿®æ­£å¾ªç¯"""
    if not state['is_logical'] and state['refinement_count'] < 1:
        state['refinement_count'] += 1
        return "refine"
    return "end"

def create_risk_graph():
    workflow = StateGraph(AgentState)
    
    workflow.add_node("planner", planning_node)
    workflow.add_node("expert_debate", lambda s: expert_node_factory("FRAUD", s)) # ç¤ºä¾‹ï¼šæ­¤å¤„å¯å¹¶è¡Œæˆ–ä¸²è”å¤šä¸ªä¸“å®¶
    workflow.add_node("arbiter", arbiter_node)
    
    workflow.set_entry_point("planner")
    workflow.add_edge("planner", "expert_debate")
    workflow.add_edge("expert_debate", "arbiter")
    
    # åŠ¨æ€è·¯ç”±ï¼šå¦‚æœé€»è¾‘ä¸è‡ªæ´½ï¼Œå›åˆ°ä¸“å®¶èŠ‚ç‚¹é‡æ–°è¾©è®º
    workflow.add_conditional_edges(
        "arbiter",
        should_continue,
        {
            "refine": "expert_debate",
            "end": END
        }
    )
    
    return workflow.compile()

# ä¿æŒå…¶ä»–è¾…åŠ©æ•°æ®ç»“æ„ä¸€è‡´
METHODOLOGIES = {
    "ASSET": {"framework": "5C + LTV Stress Test", "rules": "Standard 2024"},
    "BUSINESS": {"framework": "Cashflow Audit", "rules": "Standard 2024"},
    "DTI": {"framework": "Debt Analysis", "rules": "Standard 2024"},
    "FRAUD": {"framework": "Anti-Fraud 2.0", "rules": "Red-line check"}
}
ROLE_BRAINS = {
    "ASSET": {"name": "Asset Specialist"},
    "BUSINESS": {"name": "Business Auditor"},
    "DTI": {"name": "Financial Analyst"},
    "FRAUD": {"name": "Fraud Investigator"},
    "ARBITER": {"name": "Final Arbiter"}
}

async def run_risk_workflow(case_info: str, lang: str):
    graph = create_risk_graph()
    initial_state = {
        "case_info": case_info, "language": lang, "history": [], 
        "analysis_result": {}, "planning_instruction": "", 
        "refinement_count": 0, "is_logical": True
    }
    final_state = await graph.ainvoke(initial_state)
    return {"debate": final_state["history"], "analysis": final_state["analysis_result"]}
